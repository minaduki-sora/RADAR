{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573fe26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from datasets import load_from_disk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "# 固定参数\n",
    "L = max_len = 7  # 最大步数\n",
    "\n",
    "def reward_fn(path_idx, stop_probs, lam1=1.0, lam2=1.0):\n",
    "    n = path_idx\n",
    "    probs = np.array(stop_probs[1:], dtype=np.float32)\n",
    "    probs[0] += stop_probs[0]\n",
    "    l_arr = np.arange(len(probs))\n",
    "    reward_arr = lam1 * l_arr - lam2 * np.abs(n - l_arr)\n",
    "    expected_reward = np.sum(probs * reward_arr)\n",
    "    return expected_reward\n",
    "\n",
    "class SharedStatesDataset(Dataset):\n",
    "    def __init__(self, dataset, max_len=7):\n",
    "        self.samples = []\n",
    "        for sample in dataset:\n",
    "            state_seq = [sample[f'eagle_{i}_forward'] for i in range(1, max_len+1)]\n",
    "            stops = [sample[f'action_{i}']['stop'] for i in range(max_len+1)]\n",
    "            rewards = [reward_fn(i, stops[i]) for i in range(max_len+1)]\n",
    "            self.samples.append({\n",
    "                \"states\": state_seq,\n",
    "                \"all_rewards\": rewards,\n",
    "                \"stop_probs\": stops,\n",
    "            })\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    states = torch.tensor([x[\"states\"] for x in batch], dtype=torch.float32) # [B, 7, state_dim]\n",
    "    all_rewards = torch.tensor([x[\"all_rewards\"] for x in batch], dtype=torch.float32) # [B, 8]\n",
    "    stop_probs = [x[\"stop_probs\"] for x in batch]  # list of [8, ?]\n",
    "    return states, all_rewards, stop_probs\n",
    "\n",
    "# Actor-Critic 网络定义\n",
    "class ActorCriticNet(nn.Module):\n",
    "    def __init__(self, state_dim=10, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(state_dim, hidden_dim, batch_first=True)\n",
    "        self.actor_head = nn.Linear(hidden_dim, 2)    # action: stop/go\n",
    "        self.critic_head = nn.Linear(hidden_dim, 1)   # state value\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, 7, state_dim]\n",
    "        out, _ = self.lstm(x)  # [B, 7, hidden_dim]\n",
    "        logits = self.actor_head(out)      # [B, 7, 2]\n",
    "        values = self.critic_head(out).squeeze(-1)  # [B, 7]\n",
    "        return logits, values\n",
    "\n",
    "def sample_trajectory(logits, values, device, greedy=False):\n",
    "    \"\"\"\n",
    "    给定一个样本的logits/values，采样一条轨迹\n",
    "    返回：\n",
    "      actions: [traj_len] 0/1, traj_len<=L\n",
    "      log_probs: [traj_len]\n",
    "      state_values: [traj_len]\n",
    "      traj_len: int\n",
    "    \"\"\"\n",
    "    traj_actions = []\n",
    "    traj_log_probs = []\n",
    "    traj_values = []\n",
    "    traj_len = 0\n",
    "    for t in range(logits.size(0)):\n",
    "        prob = torch.softmax(logits[t], dim=-1)  # [2]\n",
    "        m = torch.distributions.Categorical(prob)\n",
    "        if greedy:\n",
    "            action = torch.argmax(prob).item()\n",
    "        else:\n",
    "            action = m.sample().item()\n",
    "        log_prob = m.log_prob(torch.tensor(action, device=device))\n",
    "        traj_actions.append(action)\n",
    "        traj_log_probs.append(log_prob)\n",
    "        traj_values.append(values[t])\n",
    "        traj_len += 1\n",
    "        if action == 0:  # stop\n",
    "            break\n",
    "    return traj_actions, torch.stack(traj_log_probs), torch.stack(traj_values), traj_len\n",
    "\n",
    "def evaluate_policy(model, data_loader, device, greedy=True):\n",
    "    model.eval()\n",
    "    all_lengths = []\n",
    "    with torch.no_grad():\n",
    "        for states, _, _ in data_loader:\n",
    "            states = states.to(device)\n",
    "            logits, _ = model(states)\n",
    "            B = states.size(0)\n",
    "            for b in range(B):\n",
    "                acts, _, _, traj_len = sample_trajectory(logits[b], torch.zeros(L, device=device), device, greedy=greedy)\n",
    "                all_lengths.append(traj_len)\n",
    "    return all_lengths\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    dataset_dict = load_from_disk(\"your_replay_dataset_path\")\n",
    "    train_set = dataset_dict[\"train\"]\n",
    "    test_set = dataset_dict[\"test\"]\n",
    "    train_dataset = SharedStatesDataset(train_set, max_len=L)\n",
    "    test_dataset = SharedStatesDataset(test_set, max_len=L)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    state_dim = np.array(train_dataset[0][\"states\"]).shape[1]\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = ActorCriticNet(state_dim=state_dim).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    num_epochs = 10\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    best_test_loss = float('inf')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        total_batches = 0\n",
    "        for states, all_rewards, stop_probs in train_loader:\n",
    "            states = states.to(device)\n",
    "            B = states.size(0)\n",
    "            logits, values = model(states)  # [B, 7, 2], [B, 7]\n",
    "\n",
    "            batch_loss = 0\n",
    "            for b in range(B):\n",
    "                acts, log_probs, value_traj, traj_len = sample_trajectory(logits[b], values[b], device)\n",
    "                # 对应路径长度的reward\n",
    "                reward = all_rewards[b, traj_len-1].to(device)\n",
    "                # 价值估计使用最后一个有效state\n",
    "                value = value_traj[traj_len-1]\n",
    "                advantage = reward - value\n",
    "\n",
    "                policy_loss = -log_probs.sum() * advantage.detach()\n",
    "                value_loss = advantage.pow(2)\n",
    "                loss = policy_loss + 0.5 * value_loss\n",
    "                batch_loss += loss\n",
    "\n",
    "            batch_loss = batch_loss / B\n",
    "            optimizer.zero_grad()\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += batch_loss.item()\n",
    "            total_batches += 1\n",
    "\n",
    "        avg_train_loss = total_loss / total_batches if total_batches > 0 else 0\n",
    "\n",
    "        # 测试集loss：采样轨迹，同上\n",
    "        model.eval()\n",
    "        total_loss = 0\n",
    "        total_batches = 0\n",
    "        with torch.no_grad():\n",
    "            for states, all_rewards, stop_probs in test_loader:\n",
    "                states = states.to(device)\n",
    "                B = states.size(0)\n",
    "                logits, values = model(states)\n",
    "                batch_loss = 0\n",
    "                for b in range(B):\n",
    "                    acts, log_probs, value_traj, traj_len = sample_trajectory(logits[b], values[b], device)\n",
    "                    reward = all_rewards[b, traj_len-1].to(device)\n",
    "                    value = value_traj[traj_len-1]\n",
    "                    advantage = reward - value\n",
    "                    policy_loss = -log_probs.sum() * advantage.detach()\n",
    "                    value_loss = advantage.pow(2)\n",
    "                    loss = policy_loss + 0.5 * value_loss\n",
    "                    batch_loss += loss\n",
    "                batch_loss = batch_loss / B\n",
    "                total_loss += batch_loss.item()\n",
    "                total_batches += 1\n",
    "        avg_test_loss = total_loss / total_batches if total_batches > 0 else 0\n",
    "\n",
    "        train_losses.append(avg_train_loss)\n",
    "        test_losses.append(avg_test_loss)\n",
    "        print(f\"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Test Loss: {avg_test_loss:.4f}\")\n",
    "\n",
    "        if avg_test_loss < best_test_loss:\n",
    "            best_test_loss = avg_test_loss\n",
    "            torch.save(model.state_dict(), \"actor_critic_sampled_traj_best.pt\")\n",
    "            print(f\"Best test loss updated: {best_test_loss:.4f}, model checkpoint saved.\")\n",
    "\n",
    "    # 可视化loss\n",
    "    plt.figure()\n",
    "    plt.plot(range(1, num_epochs+1), train_losses, label=\"Train Loss\")\n",
    "    plt.plot(range(1, num_epochs+1), test_losses, label=\"Test Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Actor-Critic Sampled Trajectory Loss Curve\")\n",
    "    plt.savefig(\"actor_critic_sampled_traj_loss_curve.png\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eagle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
