{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27161718",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c687a3bf",
   "metadata": {},
   "source": [
    "## 策略模型定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7db1618f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMPolicyNet(nn.Module):\n",
    "    def __init__(self, state_dim=10, lstm_hidden=128, mlp_hidden=128, num_layers=1, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.state_dim = state_dim\n",
    "        self.lstm_hidden = lstm_hidden\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=state_dim,\n",
    "            hidden_size=lstm_hidden,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(lstm_hidden + state_dim, mlp_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(mlp_hidden, 2)  # 输出2个动作的logits\n",
    "        )\n",
    "\n",
    "    def forward(self, state_seq, hidden=None):\n",
    "        \"\"\"\n",
    "        state_seq: [B, T, state_dim]  # batch, seq_len, state_dim\n",
    "        hidden: (h0, c0) tuple for LSTM initial state (optional)\n",
    "        Returns:\n",
    "            action_logits: [B, T, 2]  # 每个时刻的动作分数\n",
    "        \"\"\"\n",
    "        lstm_out, hidden = self.lstm(state_seq, hidden)  # lstm_out: [B, T, lstm_hidden]\n",
    "        logits = self.mlp(torch.cat((lstm_out, state_seq), dim=-1))  # [B, T, lstm_hidden + state_dim] -> [B, T, 2]\n",
    "        return logits, hidden\n",
    "\n",
    "    def act(self, state_seq, hidden=None, deterministic=False):\n",
    "        \"\"\"\n",
    "        用于采样动作\n",
    "        state_seq: [B, T, state_dim]\n",
    "        Returns:\n",
    "            actions: [B, T]\n",
    "        \"\"\"\n",
    "        logits, hidden = self.forward(state_seq, hidden)\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        if deterministic:\n",
    "            actions = torch.argmax(probs, dim=-1)\n",
    "        else:\n",
    "            dist = torch.distributions.Categorical(probs)\n",
    "            actions = dist.sample()\n",
    "        return actions, probs, hidden\n",
    "\n",
    "    def reset_hidden(self, batch_size):\n",
    "        \"\"\"\n",
    "        重置LSTM的隐藏状态\n",
    "        batch_size: int\n",
    "        Returns:\n",
    "            hidden: (h0, c0) tuple for LSTM initial state\n",
    "        \"\"\"\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.lstm_hidden).to(next(self.parameters()).device)\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.lstm_hidden).to(next(self.parameters()).device)\n",
    "        return (h0, c0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a156f20",
   "metadata": {},
   "source": [
    "## 加载ReplayDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bebabbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "ReplayDataset = datasets.load_from_disk(\"../data/scores_rb/shareGPT-llama3-d7-topk10-t1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1f28fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReplayDataset.set_format(\"torch\")\n",
    "# type(ReplayDataset[\"train\"][0][\"action_7\"][\"stop\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b58bc788",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eagle_1_forward': [-1.028,\n",
       "  -1.31,\n",
       "  -1.489,\n",
       "  -2.748,\n",
       "  -3.365,\n",
       "  -3.592,\n",
       "  -4.94,\n",
       "  -5.656,\n",
       "  -5.742,\n",
       "  -6.645],\n",
       " 'eagle_2_forward': [-1.049,\n",
       "  -1.315,\n",
       "  -1.489,\n",
       "  -3.074,\n",
       "  -3.375,\n",
       "  -3.592,\n",
       "  -4.56,\n",
       "  -4.926,\n",
       "  -4.996,\n",
       "  -5.34],\n",
       " 'eagle_3_forward': [-1.052,\n",
       "  -1.491,\n",
       "  -2.14,\n",
       "  -2.953,\n",
       "  -2.953,\n",
       "  -3.14,\n",
       "  -3.879,\n",
       "  -4.074,\n",
       "  -4.273,\n",
       "  -4.555],\n",
       " 'eagle_4_forward': [-1.326,\n",
       "  -1.562,\n",
       "  -2.955,\n",
       "  -3.25,\n",
       "  -3.305,\n",
       "  -3.719,\n",
       "  -3.84,\n",
       "  -3.922,\n",
       "  -4.133,\n",
       "  -4.258],\n",
       " 'eagle_5_forward': [-1.343,\n",
       "  -1.564,\n",
       "  -3.41,\n",
       "  -3.58,\n",
       "  -3.846,\n",
       "  -3.986,\n",
       "  -4.27,\n",
       "  -4.465,\n",
       "  -5.023,\n",
       "  -5.06],\n",
       " 'eagle_6_forward': [-1.343,\n",
       "  -1.565,\n",
       "  -3.426,\n",
       "  -3.852,\n",
       "  -4.664,\n",
       "  -4.68,\n",
       "  -4.734,\n",
       "  -4.85,\n",
       "  -5.36,\n",
       "  -5.51],\n",
       " 'eagle_7_forward': [-1.72,\n",
       "  -1.849,\n",
       "  -2.86,\n",
       "  -3.098,\n",
       "  -3.719,\n",
       "  -4.453,\n",
       "  -4.453,\n",
       "  -4.516,\n",
       "  -4.684,\n",
       "  -5.055],\n",
       " 'eagle_8_forward': [-2.912,\n",
       "  -2.938,\n",
       "  -3.562,\n",
       "  -3.7,\n",
       "  -3.781,\n",
       "  -3.9,\n",
       "  -3.93,\n",
       "  -3.945,\n",
       "  -4.055,\n",
       "  -4.08],\n",
       " 'action_0': {'stop': [0.00017976760864257812, 0.9997768402099609]},\n",
       " 'action_1': {'stop': [0.0003559589385986328,\n",
       "   3.0994415283203125e-06,\n",
       "   0.9995869994163513]},\n",
       " 'action_2': {'stop': [0.0003559589385986328,\n",
       "   2.7060508728027344e-05,\n",
       "   0.03680419921875,\n",
       "   0.9626894593238831]},\n",
       " 'action_3': {'stop': [0.0004425048828125,\n",
       "   8.33272933959961e-05,\n",
       "   0.11541748046875,\n",
       "   0.022125244140625,\n",
       "   0.861926794052124]},\n",
       " 'action_4': {'stop': [0.0004425048828125,\n",
       "   0.03143310546875,\n",
       "   0.08892822265625,\n",
       "   0.0260772705078125,\n",
       "   0.1861572265625,\n",
       "   0.6667017936706543]},\n",
       " 'action_5': {'stop': [0.0061492919921875,\n",
       "   0.03143310546875,\n",
       "   0.08343505859375,\n",
       "   0.0258941650390625,\n",
       "   0.19482421875,\n",
       "   0.00579071044921875,\n",
       "   0.6523411273956299]},\n",
       " 'action_6': {'stop': [0.0061492919921875,\n",
       "   0.03143310546875,\n",
       "   0.08941650390625,\n",
       "   0.10040283203125,\n",
       "   0.1148681640625,\n",
       "   0.0084991455078125,\n",
       "   0.0101318359375,\n",
       "   0.6389083862304688]},\n",
       " 'action_7': {'stop': [0.0061492919921875,\n",
       "   0.08392333984375,\n",
       "   0.03704833984375,\n",
       "   0.1397705078125,\n",
       "   0.07598876953125,\n",
       "   0.007415771484375,\n",
       "   0.021484375,\n",
       "   0.2437744140625,\n",
       "   0.3843262195587158]}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ReplayDataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4827be77",
   "metadata": {},
   "source": [
    "## 奖励函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ff4df7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# 1. 奖励函数（期望reward）\n",
    "def reward_fn(action_idx, stop_probs, lam1=1.0, lam2=0.9):\n",
    "    n = action_idx\n",
    "    probs = np.array(stop_probs[1:], dtype=np.float32)\n",
    "    probs[0] += stop_probs[0]  # 合并长度为0和1的概率\n",
    "    l_arr = np.arange(len(probs))  # [0, 1, ..., len(probs)-1]\n",
    "    reward_arr = lam1 * l_arr - lam2 * np.abs(n - l_arr)\n",
    "    expected_reward = np.sum(probs * reward_arr)\n",
    "    return expected_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4d300f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_acc_len_fn(stop_probs):\n",
    "    return np.sum(np.arange(len(stop_probs)) * np.array(stop_probs, dtype=np.float32))\n",
    "\n",
    "def avg_acc_len_fn_tensor(stop_probs:torch.Tensor):\n",
    "    \"\"\"stop_probs: [B, T]\"\"\"\n",
    "    return torch.sum(torch.arange(stop_probs.size(-1), device=stop_probs.device) * stop_probs, dim=-1)\n",
    "\n",
    "def padding_stops(stops, max_len=9):\n",
    "    \"\"\"\n",
    "    对stops进行padding，使其长度为max_len\n",
    "    0~8共9个位置\n",
    "    \"\"\"\n",
    "    padded_stops = stops + [0.0] * (max_len - len(stops))\n",
    "    return padded_stops[:max_len]  # 确保长度不超过max_len"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cf0bc0",
   "metadata": {},
   "source": [
    "## 数据集定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e09870f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "\n",
    "def normalize_probs(arr):\n",
    "    arr = np.clip(arr, 0, None)\n",
    "    row_sums = arr.sum(axis=-1, keepdims=True)\n",
    "    # 如果存在全为0的行就报错\n",
    "    if np.any(row_sums <= 1e-8):\n",
    "        raise ValueError(\"dataset error\")\n",
    "    arr = arr / row_sums\n",
    "    return arr\n",
    "\n",
    "class SharedStatesDataset(Dataset):\n",
    "    def __init__(self, dataset, max_len=7):\n",
    "        self.samples = []\n",
    "        for sample in dataset:\n",
    "            # 仅用eagle_1_forward ~ eagle_7_forward\n",
    "            state_seq = [sample[f'eagle_{i}_forward'] for i in range(1, max_len+1)]  # [7, state_dim]\n",
    "            stops = [padding_stops(sample[f'action_{i}']['stop']) for i in range(max_len+1)]\n",
    "            stops = [normalize_probs(np.array(stop)) for stop in stops]\n",
    "            # rewards = [reward_fn(i, stops[i]) for i in range(max_len+1)]\n",
    "            avg_acc_lens_op = avg_acc_len_fn(stops[-1])  # 最优平均接受长度\n",
    "            self.samples.append({\n",
    "                \"states\": state_seq,     # [7, state_dim]\n",
    "                # \"all_rewards\": rewards,  # [8]\n",
    "                \"stop_probs\": stops,\n",
    "                \"avg_acc_lens_op\": avg_acc_lens_op\n",
    "            })\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    states = torch.tensor([x[\"states\"] for x in batch], dtype=torch.float32) # [B, 7, state_dim]\n",
    "    # all_rewards = torch.tensor([x[\"all_rewards\"] for x in batch], dtype=torch.float32) # [B, 8]\n",
    "    stop_probs = torch.tensor([x[\"stop_probs\"] for x in batch], dtype=torch.float32)  # [B, 8, 9]\n",
    "    avg_acc_lens_op = torch.tensor([x[\"avg_acc_lens_op\"] for x in batch], dtype=torch.float32)  # [B]\n",
    "    return states, stop_probs, avg_acc_lens_op  \n",
    "    # return states, all_rewards\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7980041",
   "metadata": {},
   "source": [
    "## 参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1dfeb6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 7\n",
    "num_paths = max_len + 1  # 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d9a243",
   "metadata": {},
   "source": [
    "## 构造奖励矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed7d285a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTION_SEQ_MATRIX = torch.stack([\n",
    "        torch.tensor([1] * i + [0] * (num_paths - i - 1), dtype=torch.long)\n",
    "        for i in range(num_paths)\n",
    "    ])  # [8, 7]\n",
    "MASK_MATRIX = torch.tril(torch.ones((num_paths, num_paths), dtype=torch.float32))[:, :-1]  # [8, 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "106bfff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ACTION_SEQ_MATRIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a20e3db1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MASK_MATRIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "535e7d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_set = ReplayDataset[\"train\"]\n",
    "test_set = ReplayDataset[\"test\"]\n",
    "train_dataset = SharedStatesDataset(train_set, max_len=max_len)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
    "test_dataset = SharedStatesDataset(test_set, max_len=max_len)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88596f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_reward_matrix(rate=1, maxlen=7):\n",
    "    rewards = rate * torch.ones(maxlen+2, maxlen+1, maxlen+1, dtype=torch.long)\n",
    "    for al in range(9):\n",
    "        rewards[al, :, max(al-1, 0):] = -1\n",
    "    rewards = rewards[:,:,:maxlen]\n",
    "\n",
    "    action = torch.tensor(\n",
    "        [\n",
    "            [1] * i + [0] * (maxlen - i) for i in range(maxlen + 1)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    mask = torch.tril(torch.ones(maxlen+1,maxlen+1,dtype=torch.long))[:,:maxlen]\n",
    "\n",
    "    R = rewards * (2 * action - 1) * mask # [9,8,7] acclen,action_idx,reward\n",
    "    return R\n",
    "\n",
    "def make_g_matrix(reward_matrix, maxlen=7, gamma=0.9):\n",
    "    G = torch.zeros_like(reward_matrix, dtype=torch.float32)\n",
    "    G[:,:,maxlen-1] = reward_matrix[:,:,maxlen-1]  # 最后一个时间步的G值等于奖励\n",
    "    for i in reversed(range(1, maxlen)):\n",
    "        G[:,:,i-1] = reward_matrix[:,:,i-1] + gamma * G[:,:,i]\n",
    "    return G # [9,8,7] acclen,action_idx,gamma_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc85d37f",
   "metadata": {},
   "source": [
    "## 统计接受长度分布、平均接受长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0781a1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_after_first_zero(tensor):\n",
    "    \"\"\"\n",
    "    参数:\n",
    "        tensor: 输入张量，形状为[bsz, 8],仅包含整数值0或1\n",
    "    \n",
    "    返回:\n",
    "        形状为[bsz, 8]的张量,第一个0之后的所有位置被置0\n",
    "    \"\"\"\n",
    "    # 1. 找出所有0的位置\n",
    "    zeros = tensor.eq(0)\n",
    "    \n",
    "    # 2. 创建累积mask（为第一个0之后的位置标记为False）\n",
    "    # 使用cumprod将第一个0之前的True保留，之后的变为False\n",
    "    mask = torch.cat(\n",
    "        [torch.ones(zeros.size(0), 1, dtype=torch.bool, device=tensor.device),\n",
    "         ~zeros[:, :-1]], \n",
    "        dim=1\n",
    "    ).cumprod(dim=1).bool()\n",
    "    \n",
    "    return tensor * mask\n",
    "\n",
    "def find_first_zero_pos(tensor):\n",
    "    \"\"\"\n",
    "    参数:\n",
    "        tensor: 输入张量，形状为[bsz, 8]，仅包含整数值0或1\n",
    "    \n",
    "    返回:\n",
    "        形状为[bsz, 1]的张量，表示每行第一个0的位置（若没有则为8）\n",
    "    \"\"\"\n",
    "    # 步骤1: 创建布尔掩码标记0的位置\n",
    "    mask = tensor.eq(0)  # 等同于 (tensor == 0)\n",
    "    mask = mask.to(torch.long)\n",
    "    \n",
    "    # 步骤2: 在最后一维末尾添加全True的列\n",
    "    extended_mask = torch.cat(\n",
    "        [mask, torch.ones(mask.size(0), 1, dtype=torch.long, device=tensor.device)],\n",
    "        dim=1\n",
    "    )\n",
    "    \n",
    "    # 步骤3: 沿最后一维查找第一个True的位置索引\n",
    "    first_zero_pos = extended_mask.argmax(dim=1, keepdim=True)\n",
    "    return first_zero_pos\n",
    "\n",
    "from collections import defaultdict\n",
    "def cal_avg_len(model, data_loader):\n",
    "    model.eval()\n",
    "    len_dict = defaultdict(list)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for states, stop_probs, avg_acc_lens_op in data_loader:\n",
    "            # states: [B, 7, state_dim], stop_probs: [B, 8, 8], avg_acc_lens_op: [B]\n",
    "            batch_size = states.size(0)\n",
    "            hidden = model.reset_hidden(batch_size)\n",
    "            actions, _, _ = model.act(states, hidden)  # [B, T]\n",
    "            # actions中第一个0出现的位置为动作序列长度\n",
    "            action_length = find_first_zero_pos(actions)  # [B, 1]\n",
    "            expanded_index = action_length.unsqueeze(-1).expand(-1, -1, stop_probs.size(-1))  # [B, 1, max_len + 1]\n",
    "            action_stop_probs = stop_probs.gather(dim=1, index=expanded_index).squeeze(1) # [B, max_len + 1]\n",
    "            # 计算平均接受长度\n",
    "            avg_acc_lens = avg_acc_len_fn_tensor(action_stop_probs) # [B]\n",
    "            len_dict[\"avg_acc_lens\"].extend(avg_acc_lens.cpu().numpy())\n",
    "            # 计算与最优平均接受长度差值\n",
    "            optimal_avg_acc_lens = avg_acc_lens - avg_acc_lens_op\n",
    "            len_dict[\"optimal_avg_acc_lens_diff\"].extend(optimal_avg_acc_lens.cpu().numpy())\n",
    "            \n",
    "    return len_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6a8912",
   "metadata": {},
   "source": [
    "## 测试代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e12533",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_loss(model, data_loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_batches = 0\n",
    "    G = make_g_matrix(make_reward_matrix(rate=2, maxlen=max_len), maxlen=max_len, gamma=0.8)  # [9, 8, 7]\n",
    "    G = G.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for states, stop_probs, _ in data_loader:\n",
    "            states = states.to(device)\n",
    "            stop_probs = stop_probs.to(device)  # [B, 8, 9]\n",
    "            logits, _ = model(states)  # [B, 7, 2]\n",
    "\n",
    "            log_probs = nn.functional.log_softmax(logits, dim=-1)  # [B, 7, 2]\n",
    "            probs = torch.softmax(logits, dim=-1)  # [B, 7, 2]\n",
    "            action_dist = torch.distributions.Categorical(probs=probs)  # 创建Categorical分布\n",
    "            sampled_actions = action_dist.sample()  # 采样动作 [B, 7]\n",
    "            action_indices = find_first_zero_pos(sampled_actions)  # [B, 1]\n",
    "            accept_length_dist = torch.distributions.Categorical(\n",
    "                probs=stop_probs.gather(dim=1, index=action_indices.unsqueeze(-1).expand(-1, -1, stop_probs.size(-1)))\n",
    "            )\n",
    "            accept_lengths = accept_length_dist.sample()  # [B, 1]\n",
    "            # 计算回报 g = G[action_idx, accept_length, :]\n",
    "            g_matrix = G[accept_lengths.squeeze(-1).to(device), action_indices.squeeze(-1).to(device), :]  # [B, 7]\n",
    "            # 计算损失\n",
    "            action_log_probs = log_probs.gather(dim=-1, index=sampled_actions.unsqueeze(-1)).squeeze(-1)  # [B, 7]\n",
    "            loss = -(action_log_probs * g_matrix).mean()  # 平均损失\n",
    "            total_loss += loss.item()\n",
    "            total_batches += 1\n",
    "    model.train()\n",
    "    return total_loss / total_batches if total_batches > 0 else float('inf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc5632d",
   "metadata": {},
   "source": [
    "## 训练代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "df790829",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1545077/2940358126.py:39: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  stop_probs = torch.tensor([x[\"stop_probs\"] for x in batch], dtype=torch.float32)  # [B, 8, 9]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "indices should be either on cpu or on the same device as the indexed tensor (cpu)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 55\u001b[0m\n\u001b[1;32m     52\u001b[0m     total_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     54\u001b[0m avg_train_loss \u001b[38;5;241m=\u001b[39m total_loss \u001b[38;5;241m/\u001b[39m total_batches \u001b[38;5;28;01mif\u001b[39;00m total_batches \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 55\u001b[0m avg_test_loss \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avg_len = cal_avg_len(model, test_loader)\u001b[39;00m\n\u001b[1;32m     57\u001b[0m train_losses\u001b[38;5;241m.\u001b[39mappend(avg_train_loss)\n",
      "Cell \u001b[0;32mIn[16], line 24\u001b[0m, in \u001b[0;36mevaluate_loss\u001b[0;34m(model, data_loader, device)\u001b[0m\n\u001b[1;32m     22\u001b[0m accept_lengths \u001b[38;5;241m=\u001b[39m accept_length_dist\u001b[38;5;241m.\u001b[39msample()  \u001b[38;5;66;03m# [B, 1]\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# 计算回报 g = G[action_idx, accept_length, :]\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m g_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mG\u001b[49m\u001b[43m[\u001b[49m\u001b[43maccept_lengths\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction_indices\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# [B, 7]\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# 计算损失\u001b[39;00m\n\u001b[1;32m     26\u001b[0m action_log_probs \u001b[38;5;241m=\u001b[39m log_probs\u001b[38;5;241m.\u001b[39mgather(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, index\u001b[38;5;241m=\u001b[39msampled_actions\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# [B, 7]\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: indices should be either on cpu or on the same device as the indexed tensor (cpu)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "state_dim = 10\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\") \n",
    "model = LSTMPolicyNet(state_dim=state_dim).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5, weight_decay=1e-4)  \n",
    "\n",
    "action_seq_matrix = ACTION_SEQ_MATRIX\n",
    "mask_matrix = MASK_MATRIX\n",
    "\n",
    "num_epochs = 100\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "# avg_lens = []\n",
    "best_test_loss = float('inf')\n",
    "G = make_g_matrix(make_reward_matrix(rate=2, maxlen=max_len), maxlen=max_len, gamma=0.8)  # [9, 8, 7]\n",
    "G = G.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_batches = 0\n",
    "    for states, stop_probs, _ in train_loader:\n",
    "        states = states.to(device)\n",
    "        stop_probs = stop_probs.to(device)  # [B, 8, 9]\n",
    "\n",
    "        logits, _ = model(states)  # [B, 7, 2]\n",
    "        log_probs = nn.functional.log_softmax(logits, dim=-1)  # [B, 7, 2]\n",
    "        probs = torch.softmax(logits, dim=-1)  # [B, 7, 2]\n",
    "        action_dist = torch.distributions.Categorical(probs=probs)  # 创建Categorical分布\n",
    "        sampled_actions = action_dist.sample()  # 采样动作 [B, 7]\n",
    "        action_indices = find_first_zero_pos(sampled_actions)  # [B, 1]\n",
    "        accept_length_dist = torch.distributions.Categorical(\n",
    "            probs=stop_probs.gather(dim=1, index=action_indices.unsqueeze(-1).expand(-1, -1, stop_probs.size(-1)))\n",
    "        )\n",
    "        accept_lengths = accept_length_dist.sample()  # [B, 1]\n",
    "        # 计算回报 g = G[action_idx, accept_length, :]\n",
    "        g_matrix = G[accept_lengths.squeeze(-1).to(device), action_indices.squeeze(-1).to(device), :]  # [B, 7]\n",
    "        # 计算损失\n",
    "        action_log_probs = log_probs.gather(dim=-1, index=sampled_actions.unsqueeze(-1)).squeeze(-1)  # [B, 7]\n",
    "        loss = -(action_log_probs * g_matrix).mean()  # 平均损失\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        total_batches += 1\n",
    "\n",
    "    avg_train_loss = total_loss / total_batches if total_batches > 0 else 0\n",
    "    avg_test_loss = evaluate_loss(model, test_loader, device)\n",
    "    # avg_len = cal_avg_len(model, test_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    test_losses.append(avg_test_loss)\n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Test Loss: {avg_test_loss:.4f}\")\n",
    "\n",
    "    if avg_test_loss < best_test_loss:\n",
    "            best_test_loss = avg_test_loss\n",
    "            torch.save(model.state_dict(), \"lstm_policy.pt\")\n",
    "            print(f\"Best test loss updated: {best_test_loss:.4f}, model checkpoint saved.\")\n",
    "\n",
    "# 可视化loss\n",
    "plt.figure()\n",
    "plt.plot(range(1, num_epochs+1), train_losses, label=\"Train Loss\")\n",
    "plt.plot(range(1, num_epochs+1), test_losses, label=\"Test Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Training/Test Loss Curve\")\n",
    "plt.savefig(\"loss_curve.png\")\n",
    "plt.show()\n",
    "\n",
    "torch.save(model.state_dict(), \"lstm_policy.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eagle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
