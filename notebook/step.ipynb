{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c21a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "max_len = 7\n",
    "state_dim = 10\n",
    "\n",
    "class SharedStatesDataset(Dataset):\n",
    "    def __init__(self, dataset, max_len=7):\n",
    "        self.samples = []\n",
    "        for sample in dataset:\n",
    "            # 仅用eagle_1_forward ~ eagle_7_forward\n",
    "            state_seq = [sample[f'eagle_{i}_forward'] for i in range(1, max_len+1)]  # [7, state_dim]\n",
    "            stops = [sample[f'action_{i}']['stop'] for i in range(max_len+1)]\n",
    "            # rewards = [reward_fn(i, stops[i]) for i in range(max_len+1)]\n",
    "            self.samples.append({\n",
    "                \"states\": state_seq,     # [7, state_dim]\n",
    "                # \"all_rewards\": rewards,  # [8]\n",
    "                \"stop_probs\": stops,         # [8]\n",
    "            })\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    states = torch.tensor([x[\"states\"] for x in batch], dtype=torch.float32) # [B, 7, state_dim]\n",
    "    # all_rewards = torch.tensor([x[\"all_rewards\"] for x in batch], dtype=torch.float32) # [B, 8]\n",
    "    stop_probs = [x[\"stop_probs\"] for x in batch]  # [B, 8]\n",
    "    return states, stop_probs\n",
    "\n",
    "import datasets\n",
    "\n",
    "ReplayDataset = datasets.load_from_disk(\"../data/scores_rb/shareGPT-llama3-d7-topk10-t1\")\n",
    "train_set = ReplayDataset[\"train\"]\n",
    "train_dataset = SharedStatesDataset(train_set, max_len=max_len)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "class LSTMPolicyNet(nn.Module):\n",
    "    def __init__(self, state_dim=10, lstm_hidden=128, mlp_hidden=128, num_layers=1, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.state_dim = state_dim\n",
    "        self.lstm_hidden = lstm_hidden\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=state_dim,\n",
    "            hidden_size=lstm_hidden,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(lstm_hidden + state_dim, mlp_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(mlp_hidden, 2)  # 输出2个动作的logits\n",
    "        )\n",
    "\n",
    "    def forward(self, state_seq, hidden=None):\n",
    "        \"\"\"\n",
    "        state_seq: [B, T, state_dim]  # batch, seq_len, state_dim\n",
    "        hidden: (h0, c0) tuple for LSTM initial state (optional)\n",
    "        Returns:\n",
    "            action_logits: [B, T, 2]  # 每个时刻的动作分数\n",
    "        \"\"\"\n",
    "        lstm_out, hidden = self.lstm(state_seq, hidden)  # lstm_out: [B, T, lstm_hidden]\n",
    "        logits = self.mlp(torch.cat((lstm_out, state_seq), dim=-1))  # [B, T, lstm_hidden + state_dim] -> [B, T, 2]\n",
    "        return logits, hidden\n",
    "\n",
    "    def act(self, state_seq, hidden=None, deterministic=False):\n",
    "        \"\"\"\n",
    "        用于采样动作\n",
    "        state_seq: [B, T, state_dim]\n",
    "        Returns:\n",
    "            actions: [B, T]\n",
    "        \"\"\"\n",
    "        logits, hidden = self.forward(state_seq, hidden)\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        if deterministic:\n",
    "            actions = torch.argmax(probs, dim=-1)\n",
    "        else:\n",
    "            dist = torch.distributions.Categorical(probs)\n",
    "            actions = dist.sample()\n",
    "        return actions, probs, hidden\n",
    "\n",
    "    def reset_hidden(self, batch_size):\n",
    "        \"\"\"\n",
    "        重置LSTM的隐藏状态\n",
    "        batch_size: int\n",
    "        Returns:\n",
    "            hidden: (h0, c0) tuple for LSTM initial state\n",
    "        \"\"\"\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.lstm_hidden).to(next(self.parameters()).device)\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.lstm_hidden).to(next(self.parameters()).device)\n",
    "        return (h0, c0)\n",
    "\n",
    "def sample_trajectory(logits, device):\n",
    "    \"\"\"\n",
    "    logits: [L, 2]\n",
    "    返回：actions, log_probs, traj_len\n",
    "    \"\"\"\n",
    "    actions, log_probs = [], []\n",
    "    for t in range(logits.size(0)):\n",
    "        prob = torch.softmax(logits[t], dim=-1)\n",
    "        m = torch.distributions.Categorical(prob)\n",
    "        action = m.sample()\n",
    "        actions.append(action.item())\n",
    "        log_probs.append(m.log_prob(action))\n",
    "        if action.item() == 0:  # stop\n",
    "            return actions, log_probs, t+1\n",
    "    return actions, log_probs, logits.size(0)\n",
    "\n",
    "# 假定每条轨迹reward只有终点有（即reward = traj_len - 1）\n",
    "def get_stepwise_returns(traj_len, L):\n",
    "    # reward is 0 for all steps except final\n",
    "    returns = [0.0 for _ in range(traj_len-1)] + [traj_len-1]\n",
    "    return returns\n",
    "\n",
    "model = LSTMPolicyNet(state_dim=state_dim).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5, weight_decay=1e-4)  \n",
    "\n",
    "# 训练循环\n",
    "for states, stop_probs in train_loader:\n",
    "    states = states.to(device)\n",
    "    B = states.size(0)\n",
    "    logits = model(states)  # [B, 7, 2]\n",
    "    batch_loss = 0\n",
    "    for b in range(B):\n",
    "        acts, log_probs, traj_len = sample_trajectory(logits[b], device)\n",
    "        returns = get_stepwise_returns(traj_len, 7)\n",
    "        log_probs = torch.stack(log_probs) # [traj_len]\n",
    "        returns = torch.tensor(returns, dtype=torch.float32, device=device) # [traj_len]\n",
    "        loss = - (log_probs * returns).sum()\n",
    "        batch_loss += loss\n",
    "    batch_loss = batch_loss / B\n",
    "    optimizer.zero_grad()\n",
    "    batch_loss.backward()\n",
    "    optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eagle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
