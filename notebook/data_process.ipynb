{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8d44090",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_from_disk, Dataset, DatasetDict\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "def logit_stats_gpu(logits, k=10, is_softmax=False):\n",
    "    # logits: [B, V], torch.tensor on cuda\n",
    "    probs = torch.softmax(logits, dim=-1) if not is_softmax else logits\n",
    "    entropy = -(probs * (probs + 1e-12).log()).sum(dim=-1)\n",
    "    max_prob, _ = probs.max(dim=-1)\n",
    "    mean = probs.mean(dim=-1)\n",
    "    std = probs.std(dim=-1)\n",
    "    topk_probs, _ = probs.topk(k, dim=-1)\n",
    "    return entropy, max_prob, mean, std, topk_probs\n",
    "\n",
    "def batch_extract_features(lh, eh, k=10):\n",
    "    entropy_lh, max_lh, mean_lh, std_lh, topk_lh = logit_stats_gpu(lh, k)\n",
    "    entropy_eh, max_eh, mean_eh, std_eh, topk_eh = logit_stats_gpu(eh, k)\n",
    "    features = torch.cat([\n",
    "        entropy_lh.unsqueeze(1), max_lh.unsqueeze(1), mean_lh.unsqueeze(1), std_lh.unsqueeze(1), topk_lh,\n",
    "        entropy_eh.unsqueeze(1), max_eh.unsqueeze(1), mean_eh.unsqueeze(1), std_eh.unsqueeze(1), topk_eh\n",
    "    ], dim=1)\n",
    "    return features\n",
    "\n",
    "def process_split(ds, k=10, batch_size=1024, last_logit_col=\"last_logit\", egale_logit_col=\"egale_1st_forward_logit\", accept_length_col=\"accept_length\"):\n",
    "    ds.set_format(type=\"torch\", columns=[last_logit_col, egale_logit_col, accept_length_col])\n",
    "    all_features = []\n",
    "    all_accept_length = []\n",
    "    for start in tqdm(range(0, len(ds), batch_size)):\n",
    "        batch = ds[start : start + batch_size]\n",
    "        lh = batch[last_logit_col].to(\"cuda\")\n",
    "        eh = batch[egale_logit_col].to(\"cuda\")\n",
    "        features = batch_extract_features(lh, eh, k=k)\n",
    "        all_features.append(features.cpu())\n",
    "        all_accept_length.extend(batch[accept_length_col].tolist())\n",
    "    all_features = torch.cat(all_features, dim=0)\n",
    "    new_dataset = Dataset.from_dict({\n",
    "        \"features\": all_features.tolist(),\n",
    "        \"accept_length\": all_accept_length\n",
    "    })\n",
    "    return new_dataset\n",
    "\n",
    "def main():\n",
    "    input_dataset_dir = \"../data/mt-bench-llama3-d13-topk10-t0\"\n",
    "    output_dir = \"../data/mt-bench-llama3-d13-topk10-t0-cal\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    last_logit_col = \"last_logit\"\n",
    "    egale_logit_col = \"egale_1st_forward_logit\"\n",
    "    accept_length_col = \"accept_length\"\n",
    "    k = 10\n",
    "    batch_size = 1024\n",
    "\n",
    "    dataset_dict = load_from_disk(input_dataset_dir)\n",
    "    for split, ds in dataset_dict.items():\n",
    "        print(f\"Processing split: {split}\")\n",
    "        new_ds = process_split(ds, k=k, batch_size=batch_size, last_logit_col=last_logit_col, egale_logit_col=egale_logit_col, accept_length_col=accept_length_col)\n",
    "        out_json = os.path.join(output_dir, f\"dataset_{split}.json\")\n",
    "        new_ds.to_json(out_json)\n",
    "        print(f\"Saved {split} split to {out_json}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ebde3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing split: train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7 [05:50<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 58\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m split, ds \u001b[38;5;129;01min\u001b[39;00m dataset_dict\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing split: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 58\u001b[0m     new_ds \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlast_logit_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlast_logit_col\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43megale_logit_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43megale_logit_col\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_length_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_length_col\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m     out_json \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     60\u001b[0m     new_ds\u001b[38;5;241m.\u001b[39mto_json(out_json)\n",
      "Cell \u001b[0;32mIn[1], line 35\u001b[0m, in \u001b[0;36mprocess_split\u001b[0;34m(ds, k, batch_size, last_logit_col, egale_logit_col, accept_length_col)\u001b[0m\n\u001b[1;32m     33\u001b[0m lh \u001b[38;5;241m=\u001b[39m batch[last_logit_col]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     34\u001b[0m eh \u001b[38;5;241m=\u001b[39m batch[egale_logit_col]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 35\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[43mbatch_extract_features\u001b[49m(lh, eh, k\u001b[38;5;241m=\u001b[39mk)\n\u001b[1;32m     36\u001b[0m all_features\u001b[38;5;241m.\u001b[39mappend(features\u001b[38;5;241m.\u001b[39mcpu())\n\u001b[1;32m     37\u001b[0m all_accept_length\u001b[38;5;241m.\u001b[39mextend(batch[accept_length_col]\u001b[38;5;241m.\u001b[39mtolist())\n",
      "Cell \u001b[0;32mIn[1], line 35\u001b[0m, in \u001b[0;36mprocess_split\u001b[0;34m(ds, k, batch_size, last_logit_col, egale_logit_col, accept_length_col)\u001b[0m\n\u001b[1;32m     33\u001b[0m lh \u001b[38;5;241m=\u001b[39m batch[last_logit_col]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     34\u001b[0m eh \u001b[38;5;241m=\u001b[39m batch[egale_logit_col]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 35\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[43mbatch_extract_features\u001b[49m(lh, eh, k\u001b[38;5;241m=\u001b[39mk)\n\u001b[1;32m     36\u001b[0m all_features\u001b[38;5;241m.\u001b[39mappend(features\u001b[38;5;241m.\u001b[39mcpu())\n\u001b[1;32m     37\u001b[0m all_accept_length\u001b[38;5;241m.\u001b[39mextend(batch[accept_length_col]\u001b[38;5;241m.\u001b[39mtolist())\n",
      "File \u001b[0;32m_pydevd_bundle\\\\pydevd_cython.pyx:1697\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle\\\\pydevd_cython.pyx:634\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle\\\\pydevd_cython.pyx:1368\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle\\\\pydevd_cython.pyx:1311\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle\\\\pydevd_cython.pyx:494\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.conda/envs/eagle/lib/python3.9/site-packages/debugpy/_vendored/pydevd/pydevd.py:2185\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[1;32m   2182\u001b[0m             from_this_thread\u001b[38;5;241m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[1;32m   2184\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads_suspended_single_notification\u001b[38;5;241m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[0;32m-> 2185\u001b[0m         keep_suspended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrace_suspend_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_this_thread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_tracker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2187\u001b[0m frames_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[1;32m   2190\u001b[0m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/eagle/lib/python3.9/site-packages/debugpy/_vendored/pydevd/pydevd.py:2254\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, trace_suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[1;32m   2251\u001b[0m                 queue\u001b[38;5;241m.\u001b[39mput(internal_cmd)\n\u001b[1;32m   2252\u001b[0m                 wait_timeout \u001b[38;5;241m=\u001b[39m TIMEOUT_FAST\n\u001b[0;32m-> 2254\u001b[0m         \u001b[43mnotify_event\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2255\u001b[0m         notify_event\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m   2257\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/eagle/lib/python3.9/threading.py:581\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    579\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 581\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/.conda/envs/eagle/lib/python3.9/threading.py:316\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 316\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    318\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed81b300",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main1():\n",
    "    input_dataset_dir = \"../data/mt-bench-llama3-d13-topk10-t0\"\n",
    "    output_dir = \"../data/mt-bench-llama3-d13-topk10-t0-cal\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    last_logit_col = \"last_logit\"\n",
    "    egale_logit_col = \"egale_1st_forward_logit\"\n",
    "    accept_length_col = \"accept_length\"\n",
    "    k = 10\n",
    "    batch_size = 1024\n",
    "\n",
    "    dataset_dict = load_from_disk(input_dataset_dir)\n",
    "    processed_splits = {}  # 保存所有 split 的新数据集\n",
    "\n",
    "    for split, ds in dataset_dict.items():\n",
    "        print(f\"Processing split: {split}\")\n",
    "        new_ds = process_split(\n",
    "            ds, k=k, batch_size=batch_size,\n",
    "            last_logit_col=last_logit_col,\n",
    "            egale_logit_col=egale_logit_col,\n",
    "            accept_length_col=accept_length_col\n",
    "        )\n",
    "        processed_splits[split] = new_ds\n",
    "        # 如需单独保存json，可加下面两行：\n",
    "        # out_json = os.path.join(output_dir, f\"dataset_{split}.json\")\n",
    "        # new_ds.to_json(out_json)\n",
    "\n",
    "    # 保存为Arrow格式（DatasetDict），可直接用load_from_disk读取\n",
    "    output_arrow_dir = os.path.join(output_dir, \"arrow\")\n",
    "    DatasetDict(processed_splits).save_to_disk(output_arrow_dir)\n",
    "    print(f\"All splits saved to: {output_arrow_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fa992ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing split: train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:02<00:00,  2.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing split: test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  2.87it/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 6402/6402 [00:00<00:00, 679985.17 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 1601/1601 [00:00<00:00, 287091.95 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All splits saved to: ../data/mt-bench-llama3-d13-topk10-t0-cal/arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "main1()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
