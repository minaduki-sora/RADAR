{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8d44090",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_from_disk, Dataset, DatasetDict\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "def logit_stats_gpu(logits, k=10, is_softmax=False):\n",
    "    # logits: [B, V], torch.tensor on cuda\n",
    "    probs = torch.softmax(logits, dim=-1) if not is_softmax else logits\n",
    "    entropy = -(probs * (probs + 1e-12).log()).sum(dim=-1)\n",
    "    max_prob, _ = probs.max(dim=-1)\n",
    "    mean = probs.mean(dim=-1)\n",
    "    std = probs.std(dim=-1)\n",
    "    topk_probs, _ = probs.topk(k, dim=-1)\n",
    "    return entropy, max_prob, mean, std, topk_probs\n",
    "\n",
    "def batch_extract_features(lh, eh, k=10):\n",
    "    entropy_lh, max_lh, mean_lh, std_lh, topk_lh = logit_stats_gpu(lh, k)\n",
    "    entropy_eh, max_eh, mean_eh, std_eh, topk_eh = logit_stats_gpu(eh, k)\n",
    "    features = torch.cat([\n",
    "        entropy_lh.unsqueeze(1), max_lh.unsqueeze(1), mean_lh.unsqueeze(1), std_lh.unsqueeze(1), topk_lh,\n",
    "        entropy_eh.unsqueeze(1), max_eh.unsqueeze(1), mean_eh.unsqueeze(1), std_eh.unsqueeze(1), topk_eh\n",
    "    ], dim=1)\n",
    "    return features\n",
    "\n",
    "def process_split(ds, k=10, batch_size=1024, last_logit_col=\"last_logit\", egale_logit_col=\"egale_1st_forward_logit\", accept_length_col=\"accept_length\"):\n",
    "    ds.set_format(type=\"torch\", columns=[last_logit_col, egale_logit_col, accept_length_col])\n",
    "    all_features = []\n",
    "    all_accept_length = []\n",
    "    for start in tqdm(range(0, len(ds), batch_size)):\n",
    "        batch = ds[start : start + batch_size]\n",
    "        lh = batch[last_logit_col].to(\"cuda\")\n",
    "        eh = batch[egale_logit_col].to(\"cuda\")\n",
    "        features = batch_extract_features(lh, eh, k=k)\n",
    "        all_features.append(features.cpu())\n",
    "        all_accept_length.extend(batch[accept_length_col].tolist())\n",
    "    all_features = torch.cat(all_features, dim=0)\n",
    "    new_dataset = Dataset.from_dict({\n",
    "        \"features\": all_features.tolist(),\n",
    "        \"accept_length\": all_accept_length\n",
    "    })\n",
    "    return new_dataset\n",
    "\n",
    "def main():\n",
    "    input_dataset_dir = \"../data/mt-bench-llama3-d13-topk10-t0\"\n",
    "    output_dir = \"../data/mt-bench-llama3-d13-topk10-t0-cal\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    last_logit_col = \"last_logit\"\n",
    "    egale_logit_col = \"egale_1st_forward_logit\"\n",
    "    accept_length_col = \"accept_length\"\n",
    "    k = 10\n",
    "    batch_size = 1024\n",
    "\n",
    "    dataset_dict = load_from_disk(input_dataset_dir)\n",
    "    for split, ds in dataset_dict.items():\n",
    "        print(f\"Processing split: {split}\")\n",
    "        new_ds = process_split(ds, k=k, batch_size=batch_size, last_logit_col=last_logit_col, egale_logit_col=egale_logit_col, accept_length_col=accept_length_col)\n",
    "        out_json = os.path.join(output_dir, f\"dataset_{split}.json\")\n",
    "        new_ds.to_json(out_json)\n",
    "        print(f\"Saved {split} split to {out_json}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53ebde3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing split: train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:04<00:00,  1.58it/s]\n",
      "Creating json from Arrow format: 100%|██████████| 7/7 [00:00<00:00, 54.71ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved train split to ../data/mt-bench-llama3-d13-topk10-t0-cal/dataset_train.json\n",
      "Processing split: test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  2.80it/s]\n",
      "Creating json from Arrow format: 100%|██████████| 2/2 [00:00<00:00, 157.21ba/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved test split to ../data/mt-bench-llama3-d13-topk10-t0-cal/dataset_test.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed81b300",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main1():\n",
    "    input_dataset_dir = \"../data/mt-bench-llama3-d13-topk10-t0\"\n",
    "    output_dir = \"../data/mt-bench-llama3-d13-topk10-t0-cal\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    last_logit_col = \"last_logit\"\n",
    "    egale_logit_col = \"egale_1st_forward_logit\"\n",
    "    accept_length_col = \"accept_length\"\n",
    "    k = 10\n",
    "    batch_size = 1024\n",
    "\n",
    "    dataset_dict = load_from_disk(input_dataset_dir)\n",
    "    processed_splits = {}  # 保存所有 split 的新数据集\n",
    "\n",
    "    for split, ds in dataset_dict.items():\n",
    "        print(f\"Processing split: {split}\")\n",
    "        new_ds = process_split(\n",
    "            ds, k=k, batch_size=batch_size,\n",
    "            last_logit_col=last_logit_col,\n",
    "            egale_logit_col=egale_logit_col,\n",
    "            accept_length_col=accept_length_col\n",
    "        )\n",
    "        processed_splits[split] = new_ds\n",
    "        # 如需单独保存json，可加下面两行：\n",
    "        # out_json = os.path.join(output_dir, f\"dataset_{split}.json\")\n",
    "        # new_ds.to_json(out_json)\n",
    "\n",
    "    # 保存为Arrow格式（DatasetDict），可直接用load_from_disk读取\n",
    "    output_arrow_dir = os.path.join(output_dir, \"arrow\")\n",
    "    DatasetDict(processed_splits).save_to_disk(output_arrow_dir)\n",
    "    print(f\"All splits saved to: {output_arrow_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fa992ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing split: train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:02<00:00,  2.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing split: test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  2.87it/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 6402/6402 [00:00<00:00, 679985.17 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 1601/1601 [00:00<00:00, 287091.95 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All splits saved to: ../data/mt-bench-llama3-d13-topk10-t0-cal/arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "main1()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
