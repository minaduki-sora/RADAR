{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8d44090",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_from_disk, Dataset, DatasetDict\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "def logit_stats_gpu(logits, k=10, is_softmax=False):\n",
    "    # logits: [B, V], torch.tensor on cuda\n",
    "    probs = torch.softmax(logits, dim=-1) if not is_softmax else logits\n",
    "    entropy = -(probs * (probs + 1e-12).log()).sum(dim=-1)\n",
    "    max_prob, _ = probs.max(dim=-1)\n",
    "    mean = probs.mean(dim=-1)\n",
    "    std = probs.std(dim=-1)\n",
    "    topk_probs, _ = probs.topk(k, dim=-1)\n",
    "    return entropy, max_prob, mean, std, topk_probs\n",
    "\n",
    "def batch_extract_features(lh, eh, k=10):\n",
    "    entropy_lh, max_lh, mean_lh, std_lh, topk_lh = logit_stats_gpu(lh, k)\n",
    "    entropy_eh, max_eh, mean_eh, std_eh, topk_eh = logit_stats_gpu(eh, k)\n",
    "    features = torch.cat([\n",
    "        entropy_lh.unsqueeze(1), max_lh.unsqueeze(1), mean_lh.unsqueeze(1), std_lh.unsqueeze(1), topk_lh,\n",
    "        entropy_eh.unsqueeze(1), max_eh.unsqueeze(1), mean_eh.unsqueeze(1), std_eh.unsqueeze(1), topk_eh\n",
    "    ], dim=1)\n",
    "    return features\n",
    "\n",
    "def process_split(ds, k=10, batch_size=1024, last_logit_col=\"last_logit\", egale_logit_col=\"egale_1st_forward_logit\", accept_length_col=\"accept_length\"):\n",
    "    ds.set_format(type=\"torch\", columns=[last_logit_col, egale_logit_col, accept_length_col])\n",
    "    all_features = []\n",
    "    all_accept_length = []\n",
    "    for start in tqdm(range(0, len(ds), batch_size)):\n",
    "        batch = ds[start : start + batch_size]\n",
    "        lh = batch[last_logit_col].to(\"cuda\")\n",
    "        eh = batch[egale_logit_col].to(\"cuda\")\n",
    "        features = batch_extract_features(lh, eh, k=k)\n",
    "        all_features.append(features.cpu())\n",
    "        all_accept_length.extend(batch[accept_length_col].tolist())\n",
    "    all_features = torch.cat(all_features, dim=0)\n",
    "    new_dataset = Dataset.from_dict({\n",
    "        \"features\": all_features.tolist(),\n",
    "        \"accept_length\": all_accept_length\n",
    "    })\n",
    "    return new_dataset\n",
    "\n",
    "def main():\n",
    "    input_dataset_dir = \"../data/mt-bench-llama3-d13-topk10-t0\"\n",
    "    output_dir = \"../data/mt-bench-llama3-d13-topk10-t0-cal\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    last_logit_col = \"last_logit\"\n",
    "    egale_logit_col = \"egale_1st_forward_logit\"\n",
    "    accept_length_col = \"accept_length\"\n",
    "    k = 10\n",
    "    batch_size = 1024\n",
    "\n",
    "    dataset_dict = load_from_disk(input_dataset_dir)\n",
    "    for split, ds in dataset_dict.items():\n",
    "        print(f\"Processing split: {split}\")\n",
    "        new_ds = process_split(ds, k=k, batch_size=batch_size, last_logit_col=last_logit_col, egale_logit_col=egale_logit_col, accept_length_col=accept_length_col)\n",
    "        out_json = os.path.join(output_dir, f\"dataset_{split}.json\")\n",
    "        new_ds.to_json(out_json)\n",
    "        print(f\"Saved {split} split to {out_json}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ebde3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing split: train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7 [05:50<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 58\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m split, ds \u001b[38;5;129;01min\u001b[39;00m dataset_dict\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing split: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 58\u001b[0m     new_ds \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlast_logit_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlast_logit_col\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43megale_logit_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43megale_logit_col\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_length_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_length_col\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m     out_json \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     60\u001b[0m     new_ds\u001b[38;5;241m.\u001b[39mto_json(out_json)\n",
      "Cell \u001b[0;32mIn[1], line 35\u001b[0m, in \u001b[0;36mprocess_split\u001b[0;34m(ds, k, batch_size, last_logit_col, egale_logit_col, accept_length_col)\u001b[0m\n\u001b[1;32m     33\u001b[0m lh \u001b[38;5;241m=\u001b[39m batch[last_logit_col]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     34\u001b[0m eh \u001b[38;5;241m=\u001b[39m batch[egale_logit_col]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 35\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[43mbatch_extract_features\u001b[49m(lh, eh, k\u001b[38;5;241m=\u001b[39mk)\n\u001b[1;32m     36\u001b[0m all_features\u001b[38;5;241m.\u001b[39mappend(features\u001b[38;5;241m.\u001b[39mcpu())\n\u001b[1;32m     37\u001b[0m all_accept_length\u001b[38;5;241m.\u001b[39mextend(batch[accept_length_col]\u001b[38;5;241m.\u001b[39mtolist())\n",
      "Cell \u001b[0;32mIn[1], line 35\u001b[0m, in \u001b[0;36mprocess_split\u001b[0;34m(ds, k, batch_size, last_logit_col, egale_logit_col, accept_length_col)\u001b[0m\n\u001b[1;32m     33\u001b[0m lh \u001b[38;5;241m=\u001b[39m batch[last_logit_col]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     34\u001b[0m eh \u001b[38;5;241m=\u001b[39m batch[egale_logit_col]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 35\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[43mbatch_extract_features\u001b[49m(lh, eh, k\u001b[38;5;241m=\u001b[39mk)\n\u001b[1;32m     36\u001b[0m all_features\u001b[38;5;241m.\u001b[39mappend(features\u001b[38;5;241m.\u001b[39mcpu())\n\u001b[1;32m     37\u001b[0m all_accept_length\u001b[38;5;241m.\u001b[39mextend(batch[accept_length_col]\u001b[38;5;241m.\u001b[39mtolist())\n",
      "File \u001b[0;32m_pydevd_bundle\\\\pydevd_cython.pyx:1697\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle\\\\pydevd_cython.pyx:634\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle\\\\pydevd_cython.pyx:1368\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle\\\\pydevd_cython.pyx:1311\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle\\\\pydevd_cython.pyx:494\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.conda/envs/eagle/lib/python3.9/site-packages/debugpy/_vendored/pydevd/pydevd.py:2185\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[1;32m   2182\u001b[0m             from_this_thread\u001b[38;5;241m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[1;32m   2184\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads_suspended_single_notification\u001b[38;5;241m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[0;32m-> 2185\u001b[0m         keep_suspended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrace_suspend_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_this_thread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_tracker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2187\u001b[0m frames_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[1;32m   2190\u001b[0m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/eagle/lib/python3.9/site-packages/debugpy/_vendored/pydevd/pydevd.py:2254\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, trace_suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[1;32m   2251\u001b[0m                 queue\u001b[38;5;241m.\u001b[39mput(internal_cmd)\n\u001b[1;32m   2252\u001b[0m                 wait_timeout \u001b[38;5;241m=\u001b[39m TIMEOUT_FAST\n\u001b[0;32m-> 2254\u001b[0m         \u001b[43mnotify_event\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2255\u001b[0m         notify_event\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m   2257\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/eagle/lib/python3.9/threading.py:581\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    579\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 581\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/.conda/envs/eagle/lib/python3.9/threading.py:316\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 316\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    318\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed81b300",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main1():\n",
    "    input_dataset_dir = \"../data/mt-bench-llama3-d13-topk10-t0\"\n",
    "    output_dir = \"../data/mt-bench-llama3-d13-topk10-t0-cal\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    last_logit_col = \"last_logit\"\n",
    "    egale_logit_col = \"egale_1st_forward_logit\"\n",
    "    accept_length_col = \"accept_length\"\n",
    "    k = 10\n",
    "    batch_size = 1024\n",
    "\n",
    "    dataset_dict = load_from_disk(input_dataset_dir)\n",
    "    processed_splits = {}  # 保存所有 split 的新数据集\n",
    "\n",
    "    for split, ds in dataset_dict.items():\n",
    "        print(f\"Processing split: {split}\")\n",
    "        new_ds = process_split(\n",
    "            ds, k=k, batch_size=batch_size,\n",
    "            last_logit_col=last_logit_col,\n",
    "            egale_logit_col=egale_logit_col,\n",
    "            accept_length_col=accept_length_col\n",
    "        )\n",
    "        processed_splits[split] = new_ds\n",
    "        # 如需单独保存json，可加下面两行：\n",
    "        # out_json = os.path.join(output_dir, f\"dataset_{split}.json\")\n",
    "        # new_ds.to_json(out_json)\n",
    "\n",
    "    # 保存为Arrow格式（DatasetDict），可直接用load_from_disk读取\n",
    "    output_arrow_dir = os.path.join(output_dir, \"arrow\")\n",
    "    DatasetDict(processed_splits).save_to_disk(output_arrow_dir)\n",
    "    print(f\"All splits saved to: {output_arrow_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fa992ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing split: train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:02<00:00,  2.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing split: test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  2.87it/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 6402/6402 [00:00<00:00, 679985.17 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 1601/1601 [00:00<00:00, 287091.95 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All splits saved to: ../data/mt-bench-llama3-d13-topk10-t0-cal/arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "main1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74a5f9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk, concatenate_datasets\n",
    "dataset1 = load_from_disk(\"../data/scores_rb/shareGPT-vicuna-d7-topk10-t1-0\")\n",
    "dataset2 = load_from_disk(\"../data/scores_rb/shareGPT-vicuna-d7-topk10-t1-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83a023f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d82b6c91c8f74852b85bff1f86762e40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/107284 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bdd613f57f1430d8433d436a508315b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/26821 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = concatenate_datasets([dataset1, dataset2])\n",
    "dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "dataset.save_to_disk(\"../data/scores_rb/shareGPT-vicuna-d7-topk10-t1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d73d0cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['eagle_1_forward', 'eagle_2_forward', 'eagle_3_forward', 'eagle_4_forward', 'eagle_5_forward', 'eagle_6_forward', 'eagle_7_forward', 'eagle_8_forward', 'action_0', 'action_1', 'action_2', 'action_3', 'action_4', 'action_5', 'action_6', 'action_7'],\n",
       "        num_rows: 107284\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['eagle_1_forward', 'eagle_2_forward', 'eagle_3_forward', 'eagle_4_forward', 'eagle_5_forward', 'eagle_6_forward', 'eagle_7_forward', 'eagle_8_forward', 'action_0', 'action_1', 'action_2', 'action_3', 'action_4', 'action_5', 'action_6', 'action_7'],\n",
       "        num_rows: 26821\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "63169654",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.00038242340087890625, 0.9998638033866882],\n",
       " [0.0009708404541015625, 0.035552978515625, 0.9637311100959778],\n",
       " [0.0009708404541015625, 0.04150390625, 0.014312744140625, 0.9433665871620178],\n",
       " [0.0009708404541015625,\n",
       "  0.04486083984375,\n",
       "  0.024749755859375,\n",
       "  0.1595458984375,\n",
       "  0.7700685262680054],\n",
       " [0.0009746551513671875,\n",
       "  0.04656982421875,\n",
       "  0.024200439453125,\n",
       "  0.1602783203125,\n",
       "  0.040496826171875,\n",
       "  0.7277655601501465],\n",
       " [0.0016307830810546875,\n",
       "  0.045928955078125,\n",
       "  0.178955078125,\n",
       "  0.018096923828125,\n",
       "  0.0296630859375,\n",
       "  0.0992431640625,\n",
       "  0.6266781091690063],\n",
       " [0.0019741058349609375,\n",
       "  0.04571533203125,\n",
       "  0.1861572265625,\n",
       "  0.01043701171875,\n",
       "  0.0301971435546875,\n",
       "  0.098388671875,\n",
       "  0.038299560546875,\n",
       "  0.5888843536376953],\n",
       " [0.0021820068359375,\n",
       "  0.056121826171875,\n",
       "  0.175537109375,\n",
       "  0.01158905029296875,\n",
       "  0.034912109375,\n",
       "  0.0941162109375,\n",
       "  0.041717529296875,\n",
       "  0.038360595703125,\n",
       "  0.5456371307373047]]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x[\"stop\"] for x in [dataset[\"train\"][9][f\"action_{i}\"] for i in range(8)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8d1c9c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['eagle_1_forward', 'eagle_2_forward', 'eagle_3_forward', 'eagle_4_forward', 'eagle_5_forward', 'eagle_6_forward', 'eagle_7_forward', 'eagle_8_forward', 'action_0', 'action_1', 'action_2', 'action_3', 'action_4', 'action_5', 'action_6', 'action_7'],\n",
       "        num_rows: 69463\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['eagle_1_forward', 'eagle_2_forward', 'eagle_3_forward', 'eagle_4_forward', 'eagle_5_forward', 'eagle_6_forward', 'eagle_7_forward', 'eagle_8_forward', 'action_0', 'action_1', 'action_2', 'action_3', 'action_4', 'action_5', 'action_6', 'action_7'],\n",
       "        num_rows: 17366\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "dataset = datasets.load_from_disk(\"../data/scores_rb/shareGPT-ds-d7-topk10-t1-0\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d9de2f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['eagle_1_forward', 'eagle_2_forward', 'eagle_3_forward', 'eagle_4_forward', 'eagle_5_forward', 'eagle_6_forward', 'eagle_7_forward', 'eagle_8_forward', 'action_0', 'action_1', 'action_2', 'action_3', 'action_4', 'action_5', 'action_6', 'action_7'],\n",
       "        num_rows: 107284\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['eagle_1_forward', 'eagle_2_forward', 'eagle_3_forward', 'eagle_4_forward', 'eagle_5_forward', 'eagle_6_forward', 'eagle_7_forward', 'eagle_8_forward', 'action_0', 'action_1', 'action_2', 'action_3', 'action_4', 'action_5', 'action_6', 'action_7'],\n",
       "        num_rows: 26821\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "dataset = datasets.load_from_disk(\"../data/scores_rb/shareGPT-vicuna-d7-topk10-t1\")\n",
    "dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eagle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
